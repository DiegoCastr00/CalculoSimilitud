{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d96d74abf24bd29d13685801c75e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configurado en GPU 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b46ff5cf5d40adafcc0de6b223b266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:   4%|▍         | 4/100 [00:11<04:32,  2.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995042195b3645329ec12e3be3b1085a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:   8%|▊         | 8/100 [00:22<04:13,  2.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea96c0cc1c741d683e3f7f2ddcdff5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:  12%|█▏        | 12/100 [00:33<04:00,  2.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771267f77765456884534e332283a0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:  16%|█▌        | 16/100 [00:43<03:48,  2.72s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde406ba17b24058ac2c95fc16fc5b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:  20%|██        | 20/100 [00:54<03:37,  2.71s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13d75e129a8417e891ac7fdc3df0aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0:  23%|██▎       | 23/100 [01:01<03:26,  2.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 146\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    137\u001b[0m     processor_gpu0 \u001b[38;5;241m=\u001b[39m SDXLImageProcessor(\n\u001b[1;32m    138\u001b[0m         gpu_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    139\u001b[0m         csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompressPromptLite_resize768.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         end_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m  \n\u001b[1;32m    144\u001b[0m     )\n\u001b[0;32m--> 146\u001b[0m     \u001b[43mprocessor_gpu0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_images_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m, in \u001b[0;36mSDXLImageProcessor.process_images_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Procesar cuando el lote esté completo o al final\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_images) \u001b[38;5;241m==\u001b[39m batch_size \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m df_subset\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Generar imágenes en lote\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Guardar resultados\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, output_img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mimages):\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py:1403\u001b[0m, in \u001b[0;36mStableDiffusionXLImg2ImgPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, image, strength, num_inference_steps, timesteps, sigmas, denoising_start, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, aesthetic_score, negative_aesthetic_score, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ip_adapter_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ip_adapter_image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1402\u001b[0m     added_cond_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_embeds\n\u001b[0;32m-> 1403\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/accelerate/hooks.py:707\u001b[0m, in \u001b[0;36mCpuOffload.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook\u001b[38;5;241m.\u001b[39moffload()\n\u001b[0;32m--> 707\u001b[0m     \u001b[43mclear_device_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m module\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/accelerate/utils/memory.py:60\u001b[0m, in \u001b[0;36mclear_device_cache\u001b[0;34m(garbage_collection)\u001b[0m\n\u001b[1;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_cuda_available():\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SimilitudImagenes/venv/lib/python3.10/site-packages/torch/cuda/memory.py:218\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 218\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SDXLImageProcessor:\n",
    "    def __init__(self, \n",
    "                 gpu_id=0,\n",
    "                 csv_path=\"datos.csv\",\n",
    "                 input_dir=\"imagenes/resize768\",\n",
    "                 output_dir=\"imagenes/output\",\n",
    "                 num_inference_steps=25,\n",
    "                 strength=0.4,\n",
    "                 guidance_scale=7.5,\n",
    "                 start_index=0,\n",
    "                 end_index=-1,\n",
    "                 batch_size= 3):\n",
    "\n",
    "        self.gpu_id = gpu_id\n",
    "        self.csv_path = csv_path\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.strength = strength\n",
    "        self.guidance_scale = guidance_scale\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "        self.pipe = None\n",
    "        self.batch_size = 3\n",
    "    \n",
    "    def setup_pipeline(self):\n",
    "        \"\"\"Configura el pipeline SDXL optimizado para la GPU especificada\"\"\"\n",
    "        # Seleccionar GPU específica\n",
    "        torch.cuda.set_device(self.gpu_id)\n",
    "        \n",
    "        # Cargar el modelo optimizado\n",
    "        self.pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        \n",
    "        # Mover el modelo a la GPU seleccionada\n",
    "        self.pipe = self.pipe.to(f\"cuda:{self.gpu_id}\")\n",
    "        \n",
    "        # Optimizaciones de memoria y rendimiento\n",
    "        self.pipe.enable_model_cpu_offload()  # Mantiene solo los componentes necesarios en GPU\n",
    "        self.pipe.enable_vae_slicing()  # Reduce el uso de memoria del VAE\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()  # Usa xformers para atención eficiente\n",
    "        \n",
    "        print(f\"Pipeline configurado en GPU {self.gpu_id}\")\n",
    "    \n",
    "    def process_images_batch(self):\n",
    "        \"\"\"Procesa las imágenes en lotes para mayor velocidad\"\"\"\n",
    "        if self.pipe is None:\n",
    "            self.setup_pipeline()\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        start_idx = self.start_index\n",
    "        end_idx = len(df) if self.end_index == -1 else self.end_index\n",
    "        df_subset = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Procesar en lotes\n",
    "        batch_images = []\n",
    "        batch_prompts = []\n",
    "        batch_paths = []\n",
    "        \n",
    "        for idx, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=f\"GPU {self.gpu_id}\"):\n",
    "            try:\n",
    "                file_path = row['file_name']\n",
    "                prompt = row['processed_prompt']\n",
    "                image_path = self.input_dir + \"/\" + file_path\n",
    "                \n",
    "                if not os.path.exists(image_path):\n",
    "                    print(f\"Advertencia: Imagen no encontrada - {image_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Agregar al lote actual\n",
    "                init_image = Image.open(image_path).convert(\"RGB\")\n",
    "                batch_images.append(init_image)\n",
    "                batch_prompts.append(prompt)\n",
    "                batch_paths.append(file_path)\n",
    "                \n",
    "                # Procesar cuando el lote esté completo o al final\n",
    "                if len(batch_images) == self.batch_size or idx == df_subset.index[-1]:\n",
    "                    # Generar imágenes en lote\n",
    "                    outputs = self.pipe(\n",
    "                        prompt=batch_prompts,\n",
    "                        image=batch_images,\n",
    "                        num_inference_steps=self.num_inference_steps,\n",
    "                        strength=self.strength,\n",
    "                        guidance_scale=self.guidance_scale,\n",
    "               \n",
    "                    )\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    for i, output_img in enumerate(outputs.images):\n",
    "                        output_filename = f\"{os.path.splitext(os.path.basename(batch_paths[i]))[0]}_generated.png\"\n",
    "                        output_path = os.path.join(self.output_dir, output_filename)\n",
    "                        output_img.save(output_path)\n",
    "                    \n",
    "                    # Limpiar el lote\n",
    "                    batch_images = []\n",
    "                    batch_prompts = []\n",
    "                    batch_paths = []\n",
    "                    \n",
    "                    # Limpieza periódica de memoria\n",
    "                    if idx % 20 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {file_path}: {str(e)}\")\n",
    "                \n",
    "                # Reiniciar el lote en caso de error\n",
    "                batch_images = []\n",
    "                batch_prompts = []\n",
    "                batch_paths = []\n",
    "        \n",
    "        self._cleanup()\n",
    "        print(f\"Procesamiento completado en GPU {self.gpu_id}\")\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Libera recursos y memoria\"\"\"\n",
    "        if self.pipe is not None:\n",
    "            del self.pipe\n",
    "            self.pipe = None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def run_processor(params):\n",
    "    processor = SDXLImageProcessor(**params)\n",
    "    processor.process_images_batch()\n",
    "\n",
    "# Ejemplo de uso para GPU 0\n",
    "if __name__ == \"__main__\":\n",
    "    processor_gpu0 = SDXLImageProcessor(\n",
    "        gpu_id=0,\n",
    "        csv_path=\"compressPromptLite_resize768.csv\",\n",
    "        input_dir=\"imagenes/resize768\",\n",
    "        output_dir=\"imagenes/output_gpu0\",\n",
    "        start_index=0,\n",
    "        end_index=100  \n",
    "    )\n",
    "    \n",
    "    processor_gpu0.process_images_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from IPython.display import display\n",
    "\n",
    "# # Cargar el dataset\n",
    "# df = pd.read_csv('too.csv')  \n",
    "\n",
    "# example_row = df.iloc[2]\n",
    "# file_path = \"imagenes/resizeSD/\" + example_row['file_name']\n",
    "# prompt = example_row['generated_prompt']\n",
    "\n",
    "# print(f\"Procesando: {file_path}\")\n",
    "# print(f\"Prompt: {prompt}\")\n",
    "\n",
    "# # Cargar el modelo\n",
    "# pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     variant=\"fp16\",\n",
    "#     use_safetensors=True\n",
    "# )\n",
    "\n",
    "# # Mover a GPU si está disponible\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipe = pipe.to(device)\n",
    "\n",
    "# # Función para procesar una imagen\n",
    "# def process_image(image_path, prompt, strength=0.75, guidance_scale=7.5, num_inference_steps=50):\n",
    "#     \"\"\"\n",
    "#     Procesa una imagen con SDXL usando image-to-image.\n",
    "    \n",
    "#     Parámetros:\n",
    "#     - image_path: Ruta a la imagen de entrada\n",
    "#     - prompt: Texto que guía la generación\n",
    "#     - strength: Qué tanto se modificará la imagen (0-1)\n",
    "#     - guidance_scale: Cuánto seguir el prompt (valores típicos: 7-9)\n",
    "#     - num_inference_steps: Número de pasos de inferencia\n",
    "    \n",
    "#     Devuelve la imagen generada\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Cargar y preparar la imagen\n",
    "#         init_image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "#         # Mostrar imagen original\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "#         plt.imshow(init_image)\n",
    "#         plt.title(\"Imagen Original\")\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "#         # Generar la nueva imagen\n",
    "#         image = pipe(\n",
    "#             prompt=prompt,\n",
    "#             image=init_image,\n",
    "#             strength=strength,\n",
    "#             guidance_scale=guidance_scale,\n",
    "#             num_inference_steps=num_inference_steps,\n",
    "#         ).images[0]\n",
    "        \n",
    "#         # Mostrar la imagen generada\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "#         plt.imshow(image)\n",
    "#         plt.title(f\"Imagen Generada\\nStrength: {strength}, Guidance: {guidance_scale}, Steps: {num_inference_steps}\")\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "#         return image\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error al procesar la imagen: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Probar con diferentes parámetros para ver el efecto\n",
    "# strengths = [0.4, 0.5]  # Cuánto modificar la imagen original\n",
    "# guidance_scales = [7.5, 9.0]  # Qué tanto seguir el prompt\n",
    "\n",
    "# for strength in strengths:\n",
    "#     for guidance in guidance_scales:\n",
    "#         print(f\"\\nPrueba con strength={strength}, guidance_scale={guidance}\")\n",
    "#         result = process_image(\n",
    "#             file_path, \n",
    "#             prompt, \n",
    "#             strength=strength,\n",
    "#             guidance_scale=guidance,\n",
    "#             num_inference_steps=50\n",
    "#         )\n",
    "\n",
    "# # Función para procesar varias imágenes del dataset\n",
    "# def process_dataset_sample(df, num_samples=3, strength=0.75, guidance_scale=8.5, steps=50):\n",
    "#     \"\"\"\n",
    "#     Procesa varias imágenes del dataset\n",
    "#     \"\"\"\n",
    "#     # Tomar algunas muestras aleatorias\n",
    "#     sample_df = df.sample(num_samples) if len(df) > num_samples else df\n",
    "    \n",
    "#     for idx, row in sample_df.iterrows():\n",
    "#         print(f\"\\nProcesando imagen {idx+1}/{len(sample_df)}\")\n",
    "#         file_path = row['file_name']\n",
    "#         prompt = row['generated_prompt']\n",
    "        \n",
    "#         print(f\"Archivo: {file_path}\")\n",
    "#         print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "#         result = process_image(\n",
    "#             file_path,\n",
    "#             prompt,\n",
    "#             strength=strength,\n",
    "#             guidance_scale=guidance_scale,\n",
    "#             num_inference_steps=steps\n",
    "#         )\n",
    "\n",
    "# recommended_params = {\n",
    "#     'strength': 0.85,         # Alto para permitir cambios significativos\n",
    "#     'guidance_scale': 9.0,    # Fuerte adherencia al prompt\n",
    "#     'num_steps': 60           # Más pasos para mejor calidad\n",
    "# }\n",
    "\n",
    "# print(\"\\n--- Prueba con parámetros recomendados ---\")\n",
    "# print(f\"Strength: {recommended_params['strength']}\")\n",
    "# print(f\"Guidance Scale: {recommended_params['guidance_scale']}\")\n",
    "# print(f\"Steps: {recommended_params['num_steps']}\")\n",
    "\n",
    "# result = process_image(\n",
    "#     file_path,\n",
    "#     prompt,\n",
    "#     strength=recommended_params['strength'],\n",
    "#     guidance_scale=recommended_params['guidance_scale'],\n",
    "#     num_inference_steps=recommended_params['num_steps']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
