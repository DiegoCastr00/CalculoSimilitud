{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Available GPUs: 2\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "GPU 1: NVIDIA GeForce RTX 3090\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc56c2b15e82467fbf56cc88220a09ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n",
      "\n",
      "--- Processing Image-to-Image Prompts ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 27 prompts in image-to-image mode\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def setup_mistral_pipeline():\n",
    "    \"\"\"\n",
    "    Configure the Mistral-Nemo model pipeline with multi-GPU acceleration\n",
    "    \"\"\"\n",
    "    # Configurar Accelerator para usar múltiples GPUs\n",
    "    # Puedes crear un archivo de configuración o pasar los argumentos directamente\n",
    "    accelerator = Accelerator(split_batches=True, device_placement=True)\n",
    "    device = accelerator.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Verificar GPUs disponibles\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"Available GPUs: {gpu_count}\")\n",
    "        for i in range(gpu_count):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Cargar modelo y tokenizer\n",
    "    model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    # Configurar para dividir el modelo entre GPUs\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"  # Permite que transformers distribuya el modelo entre las GPUs disponibles\n",
    "    )\n",
    "\n",
    "    print(f\"Model device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'Not available'}\")\n",
    "\n",
    "    # Preparar modelo y tokenizer con Accelerator\n",
    "    model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n",
    "    # Crear pipeline de generación de texto\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        device_map=\"auto\"  # Usar todas las GPUs disponibles\n",
    "    )\n",
    "\n",
    "    return text_pipeline, accelerator\n",
    "\n",
    "    \n",
    "def process_batch(pipeline, csv_path, output_path, batch_size=100, mode=\"text-to-image\"):\n",
    "    \"\"\"\n",
    "    Process a batch of images in parallel using both GPUs\n",
    "    \"\"\"\n",
    "    # Cargar datos\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Tomar una muestra representativa de cada género\n",
    "    genres = df['genre'].unique()\n",
    "    samples_per_genre = batch_size // len(genres)\n",
    "    \n",
    "    sample_frames = []\n",
    "    for genre in genres:\n",
    "        genre_df = df[df['genre'] == genre]\n",
    "        samples = min(samples_per_genre, len(genre_df))\n",
    "        sample_frames.append(genre_df.sample(n=samples, random_state=42))\n",
    "    \n",
    "    sample_df = pd.concat(sample_frames).reset_index(drop=True)\n",
    "    \n",
    "    # Procesar prompts en lotes para aprovechar ambas GPUs\n",
    "    batch_size = 4  # Puedes ajustar este valor según la capacidad de tus GPUs\n",
    "    prompts = []\n",
    "    \n",
    "    for i in range(0, len(sample_df), batch_size):\n",
    "        batch = sample_df.iloc[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(sample_df) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        batch_inputs = []\n",
    "        for _, row in batch.iterrows():\n",
    "            # Preparar los prompts para este lote\n",
    "            system_prompt =  \"\"\"\n",
    "            You are an expert in art and image generation with Stable Diffusion in image-to-image mode.\n",
    "            Your task is to create a prompt that guides Stable Diffusion to modify an existing image\n",
    "            while maintaining certain elements but changing others to create interesting variations.\n",
    "            \n",
    "            The prompt should:\n",
    "            1. Specify which elements to preserve (general style, theme)\n",
    "            2. Indicate which elements to modify (colors, perspective, details)\n",
    "            3. Suggest concrete but subtle transformations\n",
    "            4. Maintain the essence of the original genre and artist\n",
    "            \n",
    "            Use descriptive and specific language. No more than 75 words.\n",
    "            \"\"\"\n",
    "            \n",
    "            art_context = f\"\"\"\n",
    "            Genre: {row['genre']}\n",
    "            Artist: {row['artist']}\n",
    "            Title: {row['painting_name']}\n",
    "            Description: {row['description']}\n",
    "            \"\"\"\n",
    "            \n",
    "            full_prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{art_context}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "            batch_inputs.append(full_prompt)\n",
    "        \n",
    "        # Generar respuestas para todo el lote en paralelo\n",
    "        outputs = pipeline(\n",
    "            batch_inputs,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        \n",
    "        # Extraer y limpiar las respuestas generadas\n",
    "        for output in outputs:\n",
    "            generated_text = output[0]['generated_text']\n",
    "            if \"<|im_end|>\" in generated_text:\n",
    "                generated_text = generated_text.split(\"<|im_end|>\")[0]\n",
    "            prompts.append(generated_text.strip())\n",
    "        \n",
    "        # Liberar memoria cada cierto número de lotes\n",
    "        if torch.cuda.is_available() and (i + batch_size) % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    sample_df['sd_prompt'] = prompts\n",
    "    \n",
    "    # Guardar resultados\n",
    "    sample_df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed {len(sample_df)} prompts in {mode} mode\")\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "def main():\n",
    "    # Configurar el pipeline del modelo con aceleración multi-GPU\n",
    "    pipeline, accelerator = setup_mistral_pipeline()\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Para image-to-image\n",
    "        print(\"\\n--- Processing Image-to-Image Prompts ---\")\n",
    "        image_to_image_df = process_batch(\n",
    "            pipeline,\n",
    "            \"dataset.csv\", \n",
    "            \"prompts_image_to_image.csv\",\n",
    "            batch_size=30, \n",
    "            mode=\"image-to-image\"\n",
    "        )\n",
    "        \n",
    "    finally:\n",
    "        # Limpiar memoria de GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
