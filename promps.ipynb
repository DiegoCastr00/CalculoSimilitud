{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando generador de prompts para SDXL Refiner 1.0\n",
      "Archivo de entrada: stratify.csv\n",
      "Utilizando 2 GPUs con tamaño de batch: 50\n",
      "Guardando progreso cada 50 filas\n",
      "Procesando un máximo de 200 filas para prueba\n",
      "Verificando archivo de entrada...\n",
      "Limitando a 200 filas para prueba\n",
      "Total de filas a procesar: 200\n",
      "GPU 0 procesará filas 0 a 99\n",
      "GPU 1 procesará filas 100 a 199\n",
      "GPU 1 - Procesando batch 1/2\n",
      "GPU 0 - Procesando batch 1/2\n",
      "GPU 0 - Procesado 1/50 del batch actual (total: 1)\n",
      "Ejemplo de prompt generado: Transmute a vibrant still life with a white cockat...\n",
      "GPU 1 - Procesado 101/50 del batch actual (total: 101)\n",
      "Ejemplo de prompt generado: Revolutionize the naive primitivism of Fernando Bo...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 300\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal de filas procesadas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(combined_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 261\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m combined_output \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_output.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Procesar en paralelo\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_gpus) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    262\u001b[0m     futures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_gpus):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/concurrent/futures/process.py:775\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "\n",
    "def get_system_prompt(change_level):\n",
    "    \"\"\"Obtener un prompt de sistema detallado según el nivel de cambio\"\"\"\n",
    "    base_prompt = \"\"\"You are an expert in image-to-image modification with Stable Diffusion XL.\n",
    "Create a prompt that produces a {change_level} level of change compared to the original image.\n",
    "Focus on creating a high-quality, detailed prompt for SDXL Refiner 1.0.\n",
    "Do not include references to the original artist, painting name, or description in the output.\"\"\"\n",
    "\n",
    "    level_prompts = {\n",
    "    \"subtle\": \"\"\"\n",
    "For this SUBTLE modification:\n",
    "- Maintain the exact composition, subject matter, and artistic intention\n",
    "- Make minimal adjustments to color temperature, lighting intensity, or texture details\n",
    "- Ensure the modified image would be recognized as nearly identical to the original\n",
    "- Focus on enhancing rather than changing elements\n",
    "- Consider appropriate techniques like increasing detail, adjusting contrast, or enhancing textures\n",
    "\n",
    "Keywords to consider: refine, enhance, subtle shift, gentle adjustment, nuanced change, detailed, crisp, high-quality\"\"\",\n",
    "    \n",
    "    \"moderate\": \"\"\"\n",
    "For this MODERATE modification:\n",
    "- Keep the main composition and subject recognizable\n",
    "- Transform color schemes, lighting conditions, or artistic techniques\n",
    "- Add or modify secondary elements while preserving primary subjects\n",
    "- Create a clear visual difference while maintaining the artwork's essence\n",
    "- Consider time of day changes, season shifts, or stylistic reinterpretations\n",
    "\n",
    "Keywords to consider: transform, shift, reinterpret, reimagine, alternative take, artistic variation\"\"\",\n",
    "    \n",
    "    \"radical\": \"\"\"\n",
    "For this RADICAL modification:\n",
    "- Completely transform the artistic style, era, or medium\n",
    "- Dramatically alter color palette, composition, or perspective\n",
    "- Recontextualize the subject matter in a boldly different setting\n",
    "- Create a new artistic vision that only conceptually relates to the original\n",
    "- Consider genre shifts, opposing aesthetics, or unexpected conceptual fusions\n",
    "\n",
    "Keywords to consider: revolutionize, transpose, transmute, overhaul, profound transformation, reimagined universe\"\"\"\n",
    "    }\n",
    "    \n",
    "    return base_prompt.format(change_level=change_level) + level_prompts[change_level] + \"\\n\\nCreate a detailed, vibrant prompt with descriptive adjectives. Maximum 75 words. Avoid words like 'painting' or 'artwork'.\"\n",
    "\n",
    "def generate_prompt(model, tokenizer, row, device_id=0):\n",
    "    \"\"\"Genera un prompt para Stable Diffusion usando el modelo LLM\"\"\"\n",
    "    # Usar el nivel de cambio desde la columna 'category' de cada fila\n",
    "    change_level = row['category'].lower()\n",
    "    system_prompt = get_system_prompt(change_level)\n",
    "    \n",
    "    # Información contextual de la pintura\n",
    "    art_context = f\"\"\"Genre: {row['genre']}\n",
    "Artist: {row['artist']}\n",
    "Title: {row['painting_name']}\n",
    "Description: {row['description']}\n",
    "Change level: {change_level}\n",
    "\n",
    "Generate a prompt for Stable Diffusion XL Refiner 1.0 to create a variation of this artwork.\"\"\"\n",
    "\n",
    "    # Formato para Qwen2.5-7B-Instruct-1M\n",
    "    full_prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{art_context}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    with torch.cuda.device(device_id):\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(f\"cuda:{device_id}\")\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)  # Cambiado a False para capturar los tokens especiales\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente (correctamente)\n",
    "    assistant_start_tag = \"<|im_start|>assistant\\n\"\n",
    "    assistant_end_tag = \"<|im_end|>\"\n",
    "    \n",
    "    if assistant_start_tag in response:\n",
    "        # Obtener el texto después del tag de inicio del asistente\n",
    "        assistant_part = response.split(assistant_start_tag)[1]\n",
    "        # Si hay tag de final, cortar ahí\n",
    "        if assistant_end_tag in assistant_part:\n",
    "            assistant_response = assistant_part.split(assistant_end_tag)[0].strip()\n",
    "        else:\n",
    "            assistant_response = assistant_part.strip()\n",
    "    else:\n",
    "        # Fallback: tratar de obtener solo la respuesta nueva\n",
    "        try:\n",
    "            input_text_length = len(tokenizer.encode(full_prompt))\n",
    "            assistant_response = tokenizer.decode(outputs[0][input_text_length:], skip_special_tokens=True).strip()\n",
    "        except:\n",
    "            assistant_response = \"Error extrayendo la respuesta del modelo.\"\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "def process_batch(model, tokenizer, batch_df, device_id, output_file, processed_so_far=0, save_interval=50):\n",
    "    \"\"\"Procesa un batch de datos en la GPU especificada y actualiza el archivo de salida\"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    for i, row in batch_df.iterrows():\n",
    "        try:\n",
    "            prompt_text = generate_prompt(model, tokenizer, row, device_id)\n",
    "            # Verificar si el prompt generado parece contener el formato completo\n",
    "            if \"system\\n\" in prompt_text or \"user\\n\" in prompt_text:\n",
    "                print(f\"⚠️ Advertencia: Prompt completo detectado en fila {i + processed_so_far}. Limpiando...\")\n",
    "                # Intento de limpieza adicional\n",
    "                if \"<|im_start|>assistant\\n\" in prompt_text:\n",
    "                    prompt_text = prompt_text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "                    if \"<|im_end|>\" in prompt_text:\n",
    "                        prompt_text = prompt_text.split(\"<|im_end|>\")[0]\n",
    "                # Eliminamos cualquier texto que contenga los marcadores de sistema o usuario\n",
    "                lines = [line for line in prompt_text.split(\"\\n\") if \"system\" not in line and \"user\" not in line]\n",
    "                prompt_text = \"\\n\".join(lines)\n",
    "            \n",
    "            prompts.append(prompt_text)\n",
    "            \n",
    "            # Verificar si debemos actualizar el archivo basado en el intervalo\n",
    "            current_processed = i + 1\n",
    "            absolute_processed = processed_so_far + current_processed\n",
    "            \n",
    "            if current_processed % save_interval == 0 or current_processed == len(batch_df):\n",
    "                print(f\"GPU {device_id} - Processed {absolute_processed} rows total\")\n",
    "                \n",
    "                # Actualizar el archivo con las filas procesadas hasta ahora\n",
    "                if os.path.exists(output_file):\n",
    "                    # Leer el archivo existente y agregar las nuevas filas\n",
    "                    existing_df = pd.read_csv(output_file)\n",
    "                    new_rows_df = batch_df.iloc[:current_processed].copy()\n",
    "                    new_rows_df['sdxl_prompt'] = prompts\n",
    "                    updated_df = pd.concat([existing_df, new_rows_df])\n",
    "                    updated_df.to_csv(output_file, index=False)\n",
    "                    print(f\"Archivo actualizado: {output_file} (total: {len(updated_df)} filas)\")\n",
    "                else:\n",
    "                    # Crear el archivo por primera vez\n",
    "                    new_rows_df = batch_df.iloc[:current_processed].copy()\n",
    "                    new_rows_df['sdxl_prompt'] = prompts\n",
    "                    new_rows_df.to_csv(output_file, index=False)\n",
    "                    print(f\"Archivo creado: {output_file} ({len(new_rows_df)} filas)\")\n",
    "            \n",
    "            elif i % 10 == 0:\n",
    "                print(f\"GPU {device_id} - Procesado {i+1}/{len(batch_df)} del batch actual (total: {absolute_processed})\")\n",
    "                # Mostrar un ejemplo de prompt limpio para verificación\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"ERROR: {str(e)}\"\n",
    "            print(f\"Error en fila {i + processed_so_far}: {error_msg}\")\n",
    "            prompts.append(error_msg)\n",
    "    \n",
    "    # Agregamos los prompts generados para todas las filas del batch\n",
    "    batch_df['sdxl_prompt'] = prompts\n",
    "    \n",
    "    # Devolver el DataFrame procesado y el número de filas procesadas\n",
    "    return batch_df, len(batch_df)\n",
    "\n",
    "def process_csv_chunk(input_file, output_file, device_id, start_idx, end_idx, batch_size=50, save_interval=50):\n",
    "    \"\"\"Procesa un segmento del CSV en la GPU especificada y guarda en un solo archivo por GPU\"\"\"\n",
    "    # Carga el modelo en la GPU específica\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "    \n",
    "    with torch.cuda.device(device_id):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=f\"cuda:{device_id}\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Lee solo el segmento relevante del CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "    chunk = df.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    # Procesar en batches para manejar grandes volúmenes de datos\n",
    "    total_batches = (len(chunk) + batch_size - 1) // batch_size\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Si el archivo de salida ya existe (de ejecuciones anteriores), lo eliminamos\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        print(f\"Eliminado archivo previo: {output_file}\")\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        batch_start = batch_num * batch_size\n",
    "        batch_end = min(batch_start + batch_size, len(chunk))\n",
    "        batch_df = chunk.iloc[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"GPU {device_id} - Procesando batch {batch_num+1}/{total_batches}\")\n",
    "        processed_batch, batch_processed = process_batch(\n",
    "            model, tokenizer, batch_df, device_id, output_file, \n",
    "            processed_so_far=processed_count, save_interval=save_interval\n",
    "        )\n",
    "        \n",
    "        # Actualizar el archivo de salida con este batch\n",
    "        if batch_num == 0:\n",
    "            # Para el primer batch, creamos el archivo\n",
    "            processed_batch.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            # Para batches subsiguientes, actualizamos el archivo existente\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            updated_df = pd.concat([existing_df, processed_batch])\n",
    "            updated_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        processed_count += batch_processed\n",
    "        print(f\"Batch {batch_num+1} completado y agregado a {output_file} (total: {processed_count} filas)\")\n",
    "        \n",
    "        # Limpiar memoria después de cada batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def main():\n",
    "    # Configuración\n",
    "    input_file = \"stratify.csv\"  # Cambiar al nombre de tu archivo CSV\n",
    "    output_dir = \"resultados_sdxl\"\n",
    "    num_gpus = 2  # Número de GPUs a utilizar\n",
    "    batch_size = 50  # Tamaño del batch\n",
    "    save_interval = 50  # Guardar avances cada 50 filas\n",
    "    max_rows = 200  # Limitar a 200 filas en total para pruebas\n",
    "    \n",
    "    # Validación inicial\n",
    "    print(\"Iniciando generador de prompts para SDXL Refiner 1.0\")\n",
    "    print(f\"Archivo de entrada: {input_file}\")\n",
    "    print(f\"Utilizando {num_gpus} GPUs con tamaño de batch: {batch_size}\")\n",
    "    print(f\"Guardando progreso cada {save_interval} filas\")\n",
    "    print(f\"Procesando un máximo de {max_rows} filas para prueba\")\n",
    "    print(\"Verificando archivo de entrada...\")\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Leer el CSV para obtener el número total de filas\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Limitar a max_rows para pruebas\n",
    "    if max_rows > 0 and len(df) > max_rows:\n",
    "        df = df.iloc[:max_rows]\n",
    "        print(f\"Limitando a {max_rows} filas para prueba\")\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    print(f\"Total de filas a procesar: {total_rows}\")\n",
    "    \n",
    "\n",
    "    # Dividir el trabajo entre las GPUs\n",
    "    rows_per_gpu = total_rows // num_gpus\n",
    "    \n",
    "    # Definir los archivos de salida\n",
    "    output_files = [os.path.join(output_dir, f\"output_gpu{i}.csv\") for i in range(num_gpus)]\n",
    "    combined_output = os.path.join(output_dir, \"combined_output.csv\")\n",
    "    \n",
    "    # Procesar en paralelo\n",
    "    with ProcessPoolExecutor(max_workers=num_gpus) as executor:\n",
    "        futures = []\n",
    "        for i in range(num_gpus):\n",
    "            start_idx = i * rows_per_gpu\n",
    "            end_idx = (i + 1) * rows_per_gpu if i < num_gpus - 1 else total_rows\n",
    "            \n",
    "            print(f\"GPU {i} procesará filas {start_idx} a {end_idx-1}\")\n",
    "            \n",
    "            # Enviar el trabajo a cada GPU\n",
    "            future = executor.submit(\n",
    "                process_csv_chunk, \n",
    "                input_file, \n",
    "                output_files[i], \n",
    "                i,  # ID de la GPU\n",
    "                start_idx, \n",
    "                end_idx, \n",
    "                batch_size,\n",
    "                save_interval\n",
    "            )\n",
    "            futures.append(future)\n",
    "    \n",
    "    # Esperar a que todos los procesos terminen\n",
    "    result_files = [future.result() for future in futures]\n",
    "    \n",
    "    # Combinar resultados\n",
    "    print(\"Combinando resultados finales...\")\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in result_files])\n",
    "    combined_df.to_csv(combined_output, index=False)\n",
    "    \n",
    "    print(f\"Procesamiento completado. Resultados finales:\")\n",
    "    for i, file in enumerate(output_files):\n",
    "        file_size = os.path.getsize(file) / (1024 * 1024)  # Tamaño en MB\n",
    "        print(f\"  - GPU {i}: {file} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    combined_size = os.path.getsize(combined_output) / (1024 * 1024)  # Tamaño en MB\n",
    "    print(f\"  - Combinado: {combined_output} ({combined_size:.2f} MB)\")\n",
    "    print(f\"Total de filas procesadas: {len(combined_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
